{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAiDL Spring 2025 Induction Assignment: CoreML\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "### 1. Setting up the Environment\n",
    "- [1.1 Importing the necessary packages](#11-importing-the-necessary-packages)  \n",
    "- [1.2 Fixing the seed to ensure reproducability](#12-fixing-the-seed-to-ensure-reproducability)  \n",
    "- [1.3 Detecting the GPU](#13-detecting-the-gpu)  \n",
    "\n",
    "### 2. Setting up the Experiment\n",
    "- [2.1 Configuring the noise and loss parameters](#21-configuring-the-noise-and-loss-parameters)  \n",
    "- [2.2 Helper functions to noisify the data](#22-helper-functions-to-noisify-the-data)  \n",
    "- [2.3 Wrapping the data into dataloaders](#23-wrapping-the-data-into-dataloaders)  \n",
    "- [2.4 Defining the losses](#24-defining-the-losses) \n",
    "\n",
    "### 3. Developing the Model\n",
    "- [3.1 Building the model](#31-building-the-model)  \n",
    "- [3.2 Helper function to train the model](#32-helper-function-to-train-the-model)  \n",
    "- [3.3 Helper function to evaluate the model](#33-helper-function-to-evaluate-the-model)  \n",
    "- [3.4 The Training Process](#34-the-training-process)  \n",
    "\n",
    "### 4. Capturing the Performance\n",
    "- [4.1 Validation loss and accuracy vs epochs](#41-validation-loss-and-accuracy-vs-epochs)  \n",
    "- [4.2 Comparing the noises and losses](#42-comparing-the-noises-and-losses)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setting up the Environment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Importing the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-06T03:15:08.109078Z",
     "iopub.status.busy": "2025-04-06T03:15:08.108872Z",
     "iopub.status.idle": "2025-04-06T03:15:15.038380Z",
     "shell.execute_reply": "2025-04-06T03:15:15.037753Z",
     "shell.execute_reply.started": "2025-04-06T03:15:08.109058Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import to_rgba\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Fixing the Seed to ensure Reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T03:15:15.039574Z",
     "iopub.status.busy": "2025-04-06T03:15:15.039211Z",
     "iopub.status.idle": "2025-04-06T03:15:15.049720Z",
     "shell.execute_reply": "2025-04-06T03:15:15.048812Z",
     "shell.execute_reply.started": "2025-04-06T03:15:15.039551Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Detecting the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T03:15:15.050779Z",
     "iopub.status.busy": "2025-04-06T03:15:15.050458Z",
     "iopub.status.idle": "2025-04-06T03:15:15.107542Z",
     "shell.execute_reply": "2025-04-06T03:15:15.106625Z",
     "shell.execute_reply.started": "2025-04-06T03:15:15.050757Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") \n",
    "    scaler = torch.amp.GradScaler()\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setting up the Experiment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Configuring the Noise and Loss Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T03:15:15.108725Z",
     "iopub.status.busy": "2025-04-06T03:15:15.108394Z",
     "iopub.status.idle": "2025-04-06T03:15:15.120304Z",
     "shell.execute_reply": "2025-04-06T03:15:15.119603Z",
     "shell.execute_reply.started": "2025-04-06T03:15:15.108695Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "data_root = './data' \n",
    "\n",
    "symmetric_noise_rates = [0.0, 0.2, 0.5, 0.8] \n",
    "\n",
    "asymmetric_noise_rates = [0.0, 0.1, 0.25, 0.4] \n",
    "asymmetric_transitions = {\n",
    "    9: 1,  # Truck -> Automobile\n",
    "    2: 0,  # Bird -> Airplane\n",
    "    4: 7,  # Deer -> Horse\n",
    "    3: 5,  # Cat -> Dog\n",
    "    5: 3   # Dog -> Cat\n",
    "}\n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 128\n",
    "learning_rate = 0.01 \n",
    "weight_decay = 5e-4\n",
    "lr_gamma = 0.1  \n",
    "\n",
    "losses = ['CE', 'FL', 'NCE', 'NFL', 'APL-NCE-MAE', 'APL-NFL-MAE']\n",
    "\n",
    "fl_alpha = 1.0 \n",
    "fl_gamma = 2.0 \n",
    "apl_alpha = 1.0 \n",
    "apl_beta = 1.0   \n",
    "\n",
    "results = defaultdict(lambda: defaultdict(dict)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Helper functions to Noisify the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T03:15:15.121373Z",
     "iopub.status.busy": "2025-04-06T03:15:15.121117Z",
     "iopub.status.idle": "2025-04-06T03:15:15.133644Z",
     "shell.execute_reply": "2025-04-06T03:15:15.132930Z",
     "shell.execute_reply.started": "2025-04-06T03:15:15.121339Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def noisify_symmetric(labels, noise_rate, num_classes):\n",
    "    \"\"\"\n",
    "    Applies symmetric label noise by randomly flipping a fraction of labels to any other class.\n",
    "    \"\"\"\n",
    "    if noise_rate == 0.0:\n",
    "        return labels.copy() \n",
    "\n",
    "    noisy_labels = labels.copy()\n",
    "    num_samples = len(labels)\n",
    "    num_noisy = int(noise_rate * num_samples)\n",
    "\n",
    "    noisy_indices = np.random.choice(num_samples, num_noisy, replace=False)\n",
    "\n",
    "    for i in noisy_indices:\n",
    "        original_label = labels[i]\n",
    "        new_label = np.random.randint(0, num_classes)\n",
    "        while new_label == original_label:\n",
    "            new_label = np.random.randint(0, num_classes)\n",
    "        noisy_labels[i] = new_label\n",
    "\n",
    "    print(f\"Applied symmetric noise rate: {noise_rate:.2f}. Flipped {len(noisy_indices)} labels.\")\n",
    "    return noisy_labels\n",
    "    \n",
    "\n",
    "def noisify_asymmetric(labels, noise_rate, transitions, num_classes):\n",
    "    \"\"\"\n",
    "    Applies asymmetric label noise by flipping labels according to a transition map,\n",
    "    with a small amount of uniform noise for other non-specified classes.\n",
    "    \"\"\"\n",
    "    if noise_rate == 0.0:\n",
    "        return labels.copy() \n",
    "\n",
    "    noisy_labels = labels.copy()\n",
    "    num_samples = len(labels)\n",
    "    num_noisy = 0\n",
    "    uniform_noise_rate = noise_rate * 0.1  \n",
    "\n",
    "    for i in range(num_samples):\n",
    "        original_label = labels[i]\n",
    "        if original_label in transitions:\n",
    "            if np.random.rand() < noise_rate:\n",
    "                noisy_labels[i] = transitions[original_label]\n",
    "                num_noisy += 1\n",
    "        else:\n",
    "            if np.random.rand() < uniform_noise_rate:\n",
    "                possible_labels = [l for l in range(num_classes) if l != original_label]\n",
    "                noisy_labels[i] = np.random.choice(possible_labels)\n",
    "                num_noisy += 1\n",
    "\n",
    "    print(f\"Applied asymmetric noise rate: {noise_rate:.2f} and uniform noise rate: {uniform_noise_rate:.2f}. Flipped {num_noisy} labels.\")\n",
    "    return noisy_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Wrapping the Data into DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T03:15:15.136052Z",
     "iopub.status.busy": "2025-04-06T03:15:15.135835Z",
     "iopub.status.idle": "2025-04-06T03:15:15.150375Z",
     "shell.execute_reply": "2025-04-06T03:15:15.149627Z",
     "shell.execute_reply.started": "2025-04-06T03:15:15.136034Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
    "cifar10_std = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar10_mean, cifar10_std),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar10_mean, cifar10_std),\n",
    "])\n",
    "\n",
    "\n",
    "class NoisyCIFAR10(Dataset):\n",
    "    \"\"\"\n",
    "    A custom Dataset wrapper for CIFAR-10 that replaces ground truth labels with noisy labels while preserving access to original targets.\n",
    "    \"\"\"\n",
    "    def __init__(self, cifar_dataset, noisy_labels):\n",
    "        self.cifar_dataset = cifar_dataset\n",
    "        self.noisy_labels = noisy_labels\n",
    "        if hasattr(self.cifar_dataset, 'dataset'):\n",
    "            base_dataset = self.cifar_dataset.dataset\n",
    "            indices = self.cifar_dataset.indices\n",
    "            if isinstance(base_dataset.targets, list):\n",
    "                self.original_targets = np.array(base_dataset.targets)[indices]\n",
    "            else:\n",
    "                self.original_targets = base_dataset.targets[indices]\n",
    "        else:\n",
    "            if isinstance(self.cifar_dataset.targets, list):\n",
    "                self.original_targets = np.array(self.cifar_dataset.targets)\n",
    "            else:\n",
    "                self.original_targets = self.cifar_dataset.targets\n",
    "        if len(self.noisy_labels) != len(self.cifar_dataset):\n",
    "            raise ValueError(\"Length of noisy labels must match dataset size\")\n",
    "    def __getitem__(self, index):\n",
    "        img, _ = self.cifar_dataset[index]\n",
    "        noisy_label = self.noisy_labels[index]\n",
    "        return img, noisy_label\n",
    "    def __len__(self):\n",
    "        return len(self.cifar_dataset)\n",
    "        \n",
    "\n",
    "def get_dataloaders(noise_type, noise_rate, batch_size, data_root='./data', validation_split=0.15, num_classes=10, asymmetric_transitions=None):\n",
    "    \"\"\"\n",
    "    Creates and returns DataLoaders for training, validation, and test datasets using CIFAR-10,\n",
    "    with label noise injected into the training data (either symmetric or asymmetric).\n",
    "    \"\"\"\n",
    "    cifar_train = CIFAR10(root=data_root, train=True, download=True, transform=transform_train)\n",
    "    test_dataset = CIFAR10(root=data_root, train=False, download=True, transform=transform_test)\n",
    "    \n",
    "    if isinstance(cifar_train.targets, list):\n",
    "        original_train_labels = np.array(cifar_train.targets)\n",
    "    else:\n",
    "        original_train_labels = cifar_train.targets\n",
    "        \n",
    "    num_train = len(cifar_train)\n",
    "    indices = np.arange(num_train)\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(validation_split * num_train))\n",
    "    \n",
    "    val_indices = indices[:split]\n",
    "    train_indices = indices[split:]\n",
    "    train_subset = Subset(cifar_train, train_indices)\n",
    "    val_subset = Subset(cifar_train, val_indices)\n",
    "    train_clean_labels = original_train_labels[train_indices]\n",
    "    \n",
    "    if noise_type == 'symmetric':\n",
    "        noisy_train_labels = noisify_symmetric(train_clean_labels, noise_rate, num_classes)\n",
    "    elif noise_type == 'asymmetric':\n",
    "        noisy_train_labels = noisify_asymmetric(train_clean_labels, noise_rate, asymmetric_transitions, num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown noise type: {noise_type}\")\n",
    "        \n",
    "    train_dataset_noisy = NoisyCIFAR10(train_subset, noisy_train_labels)\n",
    "    train_loader = DataLoader(train_dataset_noisy, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size*2, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Defining the Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T11:36:47.354162Z",
     "iopub.status.busy": "2025-04-06T11:36:47.353826Z",
     "iopub.status.idle": "2025-04-06T11:36:47.434520Z",
     "shell.execute_reply": "2025-04-06T11:36:47.433402Z",
     "shell.execute_reply.started": "2025-04-06T11:36:47.354115Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss for classification tasks to address class imbalance by focusing more on hard examples with adjustable alpha and gamma.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2, reduction='mean'):\n",
    "            super().__init__()\n",
    "            self.alpha = alpha\n",
    "            self.gamma = gamma\n",
    "            self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        probs = F.softmax(inputs, dim=-1)\n",
    "        ce_loss = F.cross_entropy(inputs, targets.long(), reduction='none')\n",
    "        p_t = torch.gather(probs, 1, targets.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        modulating_factor = (1 - p_t) ** self.gamma\n",
    "        focal_loss = self.alpha * modulating_factor * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss\n",
    "criterion_fl = FocalLoss(alpha=fl_alpha, gamma=fl_gamma)\n",
    "\n",
    "\n",
    "class NormalizedCrossEntropy(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Normalized Cross-Entropy that adjusts for varying confidence across predictions.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, scale=1.0):\n",
    "        super(NormalizedCrossEntropy, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, pred, labels):\n",
    "        pred = F.log_softmax(pred, dim=1)\n",
    "        label_one_hot = torch.nn.functional.one_hot(labels, self.num_classes).float().to(self.device)\n",
    "        nce = -1 * torch.sum(label_one_hot * pred, dim=1) / (- pred.sum(dim=1))\n",
    "        return self.scale * nce.mean()\n",
    "criterion_nce = NormalizedCrossEntropy(num_classes=num_classes)\n",
    "    \n",
    "\n",
    "class NormalizedFocalLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss normalized across predictions to improve stability in imbalanced settings.\n",
    "    \"\"\"\n",
    "    def __init__(self, scale=1.0, gamma=0, num_classes=10, alpha=None, size_average=True):\n",
    "        super(NormalizedFocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.size_average = size_average\n",
    "        self.num_classes = num_classes\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        target = target.view(-1, 1)\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        normalizor = torch.sum(-1 * (1 - logpt.data.exp()) ** self.gamma * logpt, dim=1)\n",
    "        logpt = logpt.gather(1, target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = torch.autograd.Variable(logpt.data.exp())\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        loss = self.scale * loss / normalizor\n",
    "\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()\n",
    "criterion_nfl = NormalizedFocalLoss(alpha=fl_alpha, gamma=fl_gamma)\n",
    "\n",
    "\n",
    "class MAELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Mean Absolute Error loss for classification, computed between predicted probabilities and one-hot encoded ground truths.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, scale=1.0):\n",
    "        super(MAELoss, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.scale = scale\n",
    "        return\n",
    "\n",
    "    def forward(self, pred, labels):\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "        label_one_hot = torch.nn.functional.one_hot(labels, self.num_classes).float().to(self.device)\n",
    "        mae = 1. - torch.sum(label_one_hot * pred, dim=1)\n",
    "        return self.scale * mae.mean() \n",
    "criterion_mae = MAELoss(num_classes=num_classes)\n",
    "\n",
    "\n",
    "class NFLandMAE(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Adaptive loss combining Normalized Focal Loss and MAE to balance robustness and precision.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha, beta, num_classes, gamma=0.5):\n",
    "        super(NFLandMAE, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.nfl = NormalizedFocalLoss(scale=alpha, gamma=gamma, num_classes=num_classes)\n",
    "        self.mae = MAELoss(scale=beta, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, pred, labels):\n",
    "        return self.nfl(pred, labels) + self.mae(pred, labels)\n",
    "criterion_apl_nfl_mae = NFLandMAE(alpha=apl_alpha, beta=apl_beta, num_classes=num_classes)\n",
    "\n",
    "\n",
    "class NCEandMAE(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Adaptive loss combining Normalized Cross-Entropy and MAE for stable yet expressive learning.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha, beta, num_classes):\n",
    "        super(NCEandMAE, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.nce = NormalizedCrossEntropy(scale=alpha, num_classes=num_classes)\n",
    "        self.mae = MAELoss(scale=beta, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, pred, labels):\n",
    "        return self.nce(pred, labels) + self.mae(pred, labels)\n",
    "criterion_apl_nce_mae = NCEandMAE(alpha=apl_alpha, beta=apl_beta, num_classes=num_classes)\n",
    "\n",
    "\n",
    "loss_functions = {\n",
    "    'CE': criterion_ce,\n",
    "    'FL': criterion_fl,\n",
    "    'NCE': criterion_nce,\n",
    "    'NFL': criterion_nfl,\n",
    "    'APL-NCE-MAE': criterion_apl_nce_mae,\n",
    "    'APL-NFL-MAE': criterion_apl_nfl_mae,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Developing the Model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T03:15:15.167633Z",
     "iopub.status.busy": "2025-04-06T03:15:15.167444Z",
     "iopub.status.idle": "2025-04-06T03:15:15.181863Z",
     "shell.execute_reply": "2025-04-06T03:15:15.181034Z",
     "shell.execute_reply.started": "2025-04-06T03:15:15.167607Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_model(num_classes=10, pretrained=False):\n",
    "    \"\"\"\n",
    "    Builds a ResNet18 model, optionally with pretrained ImageNet weights, and replaces the final fully connected layer for classification.\n",
    "    \"\"\"\n",
    "    weights = torchvision.models.ResNet18_Weights.DEFAULT if pretrained else None\n",
    "    model = torchvision.models.resnet18(weights=weights)\n",
    "\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Helper function to Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T03:15:15.182820Z",
     "iopub.status.busy": "2025-04-06T03:15:15.182610Z",
     "iopub.status.idle": "2025-04-06T03:15:15.201738Z",
     "shell.execute_reply": "2025-04-06T03:15:15.200987Z",
     "shell.execute_reply.started": "2025-04-06T03:15:15.182803Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"\n",
    "    Trains and evaluates the model for one epoch using the given criterion, optimizer, and data loaders, returning losses and accuracies.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device, dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        if batch_idx % 100 == 99:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs} | Batch: {batch_idx+1}/{len(train_loader)} | Train Loss: {loss.item():.4f} | Train Acc: {100.*correct/total:.2f}% | Time: {elapsed_time:.2f}s')\n",
    "            \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, dtype=torch.long)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets) \n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    val_loss = running_loss / total\n",
    "    val_acc = 100. * correct / total\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'--- Epoch {epoch+1} Finished ---')\n",
    "    print(f'Val Loss: {val_loss:.4f} | Val Accuracy: {val_acc:.2f}% | Time: {elapsed_time:.2f}s')\n",
    "    print()\n",
    "\n",
    "    return epoch_loss, epoch_acc, val_loss, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Helper function to Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T03:15:15.202592Z",
     "iopub.status.busy": "2025-04-06T03:15:15.202354Z",
     "iopub.status.idle": "2025-04-06T03:15:15.216723Z",
     "shell.execute_reply": "2025-04-06T03:15:15.215940Z",
     "shell.execute_reply.started": "2025-04-06T03:15:15.202574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the given test dataset and returns the loss and accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, dtype=torch.long)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets) \n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    test_loss = running_loss / total\n",
    "    test_acc = 100. * correct / total\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Evaluation Results -- Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}% | Time: {elapsed_time:.2f}s')\n",
    "    print()\n",
    "    \n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 The Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Starting Experiments...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "symmetric_noise_rates = [0.0] \n",
    "\n",
    "asymmetric_noise_rates = [0.0] \n",
    "\n",
    "all_noise_types = {\n",
    "    'symmetric': symmetric_noise_rates,\n",
    "    'asymmetric': asymmetric_noise_rates\n",
    "}\n",
    "\n",
    "experiment_start_time = time.time()\n",
    "\n",
    "val_metrics = {\n",
    "    'symmetric': {loss_name: {rate: {'loss': [], 'acc': []} for rate in symmetric_noise_rates} for loss_name in losses},\n",
    "    'asymmetric': {loss_name: {rate: {'loss': [], 'acc': []} for rate in asymmetric_noise_rates} for loss_name in losses}\n",
    "}\n",
    "\n",
    "for noise_type, noise_rates in all_noise_types.items():\n",
    "    print(f\"\\n===== Running Experiments for Noise Type: {noise_type.upper()} =====\")\n",
    "    for loss_name in losses:\n",
    "        print(f\"\\n----- Training with Loss: {loss_name} -----\")\n",
    "        for noise_rate in noise_rates:\n",
    "            print(f\"\\n--- Noise Rate: {noise_rate:.2f} ---\")\n",
    "\n",
    "            print(\"Preparing Dataloaders...\")\n",
    "            train_loader, val_loader, test_loader = get_dataloaders(noise_type, noise_rate, batch_size, data_root, asymmetric_transitions=asymmetric_transitions)\n",
    "            print()\n",
    "\n",
    "            print(\"Initializing Model and Optimizer...\")\n",
    "            model = build_model(num_classes=num_classes).to(device)\n",
    "            criterion = loss_functions[loss_name].to(device)\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=learning_rate * 1e-2)\n",
    "            print()\n",
    "\n",
    "            print(\"Starting Training...\")\n",
    "            best_val_acc = 0.0\n",
    "            for epoch in range(num_epochs):\n",
    "                train_loss, train_acc,val_loss, val_acc = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
    "                scheduler.step() \n",
    "                val_metrics[noise_type][loss_name][noise_rate]['loss'].append(val_loss)\n",
    "                val_metrics[noise_type][loss_name][noise_rate]['acc'].append(val_acc)\n",
    "\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    torch.save(model.state_dict(), f\"best_model_{noise_type}_{loss_name}_{noise_rate:.1f}.pth\")\n",
    "\n",
    "            print(f\"--- Finished Training for {loss_name} at Noise Rate {noise_rate:.2f} ---\")\n",
    "            print(f\"Best Val Accuracy achieved: {best_val_acc:.2f}%\")\n",
    "            test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "            print(f\"Test Accuracy achieved: {test_acc:.2f}%\")\n",
    "\n",
    "            results[noise_type][loss_name][noise_rate] = test_acc\n",
    "            results_list = [{\n",
    "                'Noise Type': noise_type,\n",
    "                'Loss Function': loss_name,\n",
    "                'Noise Rate': noise_rate,\n",
    "                'Test Accuracy': test_acc\n",
    "            }]\n",
    "\n",
    "            csv_path = \"/home/user/Music/dont_touch/indic/results.csv\"\n",
    "            try:\n",
    "                existing_df = pd.read_csv(csv_path)\n",
    "                updated_df = pd.concat([existing_df, pd.DataFrame(results_list)], ignore_index=True)\n",
    "            except FileNotFoundError:\n",
    "                updated_df = pd.DataFrame(results_list)\n",
    "\n",
    "            updated_df.to_csv(csv_path, index=False)\n",
    "            print(\"✅ Partial result saved to results.csv\")\n",
    "\n",
    "            del model, optimizer, scheduler, train_loader, val_loader, test_loader\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "\n",
    "experiment_end_time = time.time()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"All Experiments Completed in {(experiment_end_time - experiment_start_time)/3600:.2f} hours\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Capturing the Performance\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Validation Loss and Accuracy vs Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T08:13:34.030552Z",
     "iopub.status.busy": "2025-04-06T08:13:34.030308Z",
     "iopub.status.idle": "2025-04-06T08:13:37.309511Z",
     "shell.execute_reply": "2025-04-06T08:13:37.308657Z",
     "shell.execute_reply.started": "2025-04-06T08:13:34.030522Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "symmetric_noise_rate = 0.2  \n",
    "asymmetric_noise_rate = 0.1 \n",
    "\n",
    "def plot_loss_function_comparison(noise_type, noise_rate, val_metrics):\n",
    "    \"\"\"\n",
    "    Creates comparison plots for multiple loss functions at a specific noise setting.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 6), dpi=120, facecolor='white')\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "    markers = ['o', 's', '^', 'D', 'v', '*']\n",
    "    line_width = 2.0\n",
    "    marker_size = 8\n",
    "    spine_width = 1.5\n",
    "    \n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    \n",
    "    for i, loss_name in enumerate(losses):\n",
    "        loss_data = val_metrics[noise_type][loss_name][noise_rate]['loss']\n",
    "        epochs = range(1, len(loss_data) + 1)\n",
    "        \n",
    "        plt.plot(\n",
    "            epochs, \n",
    "            loss_data, \n",
    "            marker=markers[i % len(markers)],\n",
    "            linestyle='-',\n",
    "            linewidth=line_width,\n",
    "            markersize=marker_size,\n",
    "            markerfacecolor=colors[i % len(colors)],\n",
    "            markeredgecolor=colors[i % len(colors)],\n",
    "            color=colors[i % len(colors)],\n",
    "            label=loss_name,\n",
    "            zorder=5\n",
    "        )\n",
    "    \n",
    "    plt.title(f'Validation Loss Comparison', fontsize=16, fontweight='bold', pad=15)\n",
    "    plt.xlabel('Epoch', fontsize=13, labelpad=10)\n",
    "    plt.ylabel('Loss', fontsize=13, labelpad=10)\n",
    "    plt.grid(axis='both', linestyle='--', alpha=0.7, zorder=0)\n",
    "    \n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['left'].set_linewidth(spine_width)\n",
    "    ax1.spines['bottom'].set_linewidth(spine_width)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=11)\n",
    "    ax1.set_facecolor('#f8f9fa')\n",
    "    \n",
    "    plt.legend(fontsize=10, frameon=True, framealpha=0.9, edgecolor='gray')\n",
    "    \n",
    "    ax2 = plt.subplot(1, 2, 2)\n",
    "    \n",
    "    for i, loss_name in enumerate(losses):\n",
    "        acc_data = val_metrics[noise_type][loss_name][noise_rate]['acc']\n",
    "        epochs = range(1, len(acc_data) + 1)\n",
    "        \n",
    "        plt.plot(\n",
    "            epochs, \n",
    "            acc_data, \n",
    "            marker=markers[i % len(markers)],\n",
    "            linestyle='-',\n",
    "            linewidth=line_width,\n",
    "            markersize=marker_size,\n",
    "            markerfacecolor=colors[i % len(colors)],\n",
    "            markeredgecolor=colors[i % len(colors)],\n",
    "            color=colors[i % len(colors)],\n",
    "            label=loss_name,\n",
    "            zorder=5\n",
    "        )\n",
    "    \n",
    "    plt.title(f'Validation Accuracy Comparison', fontsize=16, fontweight='bold', pad=15)\n",
    "    plt.xlabel('Epoch', fontsize=13, labelpad=10)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=13, labelpad=10)\n",
    "    plt.grid(axis='both', linestyle='--', alpha=0.7, zorder=0)\n",
    "    \n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    ax2.spines['left'].set_linewidth(spine_width)\n",
    "    ax2.spines['bottom'].set_linewidth(spine_width)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=11)\n",
    "    ax2.set_facecolor('#f8f9fa')\n",
    "    \n",
    "    plt.legend(fontsize=10, frameon=True, framealpha=0.9, edgecolor='gray')\n",
    "    \n",
    "    plt.suptitle(f'{noise_type.capitalize()} Noise (Rate: {noise_rate}) - Loss Function Comparison', fontsize=18, fontweight='bold', y=1.05)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.25)\n",
    "    \n",
    "    plt.savefig(f'loss_comparison_{noise_type}_rate_{noise_rate}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_loss_function_comparison('symmetric', symmetric_noise_rate, val_metrics)\n",
    "plot_loss_function_comparison('asymmetric', asymmetric_noise_rate, val_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Comparing the Noises and Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T08:13:37.310498Z",
     "iopub.status.busy": "2025-04-06T08:13:37.310256Z",
     "iopub.status.idle": "2025-04-06T08:13:37.945928Z",
     "shell.execute_reply": "2025-04-06T08:13:37.945056Z",
     "shell.execute_reply.started": "2025-04-06T08:13:37.310478Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"results.csv\")\n",
    "\n",
    "def plot_results(df, noise_type_filter, title):\n",
    "    \"\"\"\n",
    "    Plots the test accuracy against noise rates for different loss functions under a specified noise type.\n",
    "    \"\"\"\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    \n",
    "    plt.figure(figsize=(12, 7), dpi=120)\n",
    "    \n",
    "    filtered_df = df[df['Noise Type'] == noise_type_filter]\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "    line_styles = ['-', '--', '-.', ':']\n",
    "    markers = ['o', 's', '^', 'D', 'v', 'p']\n",
    "    \n",
    "    loss_types = df['Loss Function'].unique()\n",
    "    \n",
    "    for i, loss_name in enumerate(loss_types):\n",
    "        loss_df = filtered_df[filtered_df['Loss Function'] == loss_name].sort_values(by='Noise Rate')\n",
    "        if not loss_df.empty:\n",
    "            color = colors[i % len(colors)]\n",
    "            \n",
    "            plt.plot(\n",
    "                loss_df['Noise Rate'], \n",
    "                loss_df['Test Accuracy'], \n",
    "                marker=markers[i % len(markers)],\n",
    "                linestyle=line_styles[i % len(line_styles)],\n",
    "                linewidth=2.5, \n",
    "                markersize=10,\n",
    "                markeredgewidth=2,\n",
    "                markerfacecolor='white',\n",
    "                markeredgecolor=color,\n",
    "                color=color,\n",
    "                label=loss_name,\n",
    "                zorder=5\n",
    "            )\n",
    "    \n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.gca().spines['left'].set_linewidth(1.5)\n",
    "    plt.gca().spines['bottom'].set_linewidth(1.5)\n",
    "    \n",
    "    plt.title(title, fontsize=18, fontweight='bold', pad=15)\n",
    "    plt.xlabel('Noise Rate (η)', fontsize=15, fontweight='medium', labelpad=10)\n",
    "    plt.ylabel('Test Accuracy (%) on Clean Data', fontsize=15, fontweight='medium', labelpad=10)\n",
    "    \n",
    "    plt.legend(\n",
    "        fontsize=12, \n",
    "        title=\"Loss Function\",\n",
    "        title_fontsize=13,\n",
    "        frameon=True,\n",
    "        fancybox=True,\n",
    "        framealpha=0.9,\n",
    "        shadow=True,\n",
    "        loc='best'\n",
    "    )\n",
    "    \n",
    "    plt.xticks(filtered_df['Noise Rate'].unique(), fontsize=13, fontweight='medium')\n",
    "    plt.yticks(np.arange(0, 101, 10), fontsize=13, fontweight='medium')\n",
    "    \n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.6, zorder=0)\n",
    "    \n",
    "    plt.ylim(bottom=0, top=100)\n",
    "    \n",
    "    plt.gca().set_facecolor('#f9f9f9')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "if 'symmetric' in results:\n",
    "    plot_results(results_df, 'symmetric', 'CIFAR-10 Robustness to Symmetric Label Noise')\n",
    "\n",
    "if 'asymmetric' in results:\n",
    "    plot_results(results_df, 'asymmetric', 'CIFAR-10 Robustness to Asymmetric Label Noise')\n",
    "\n",
    "print(\"\\n🎯 Plotting complete.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "pytorch_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
